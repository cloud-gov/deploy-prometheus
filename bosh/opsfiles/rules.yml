# AWS IAM MFA alerts
- type: replace
  path: /instance_groups/name=prometheus/jobs/name=prometheus2/properties/prometheus/custom_rules?/-
  value:
    name: aws-mfa
    rules:
    - alert: AWSMFADisabled
      expr: aws_iam_user_mfa != 1
      labels:
        service: aws-iam
        severity: warning
      annotations:
        summary: AWS IAM user {{$labels.instance}} does not have MFA enabled
        description: Remind user {{$labels.instance}} to enable MFA
    - alert: AWSMFANotRunning
      expr: time() - push_time_seconds{job="aws_iam"} > 7200
      labels:
        service: aws-iam
        severity: warning
      annotations:
        summary: AWS IAM check is not running
        description: Check https://ci.fr.cloud.gov/teams/main/pipelines/deploy-prometheus?groups=checks for failures

# AWS RDS storage alerts
- type: replace
  path: /instance_groups/name=prometheus/jobs/name=prometheus2/properties/prometheus/custom_rules?/-
  value:
    name: aws-rds-storage
    rules:
    - alert: AWSRDSStorage
      expr: aws_rds_disk_free < (aws_rds_disk_allocated * .12)
      labels:
        service: aws-rds
        severity: warning
      annotations:
        summary: AWS RDS {{$labels.instance}} has used 88% or greater disk space
        description: Email the organization administrator for {{$labels.instance}} to inquire about provisioning a larger database.

# AWS IAM Stale Key alerts
- type: replace
  path: /instance_groups/name=prometheus/jobs/name=prometheus2/properties/prometheus/custom_rules?/-
  value:
    name: aws-iam-check-keys
    rules:
    - alert: OperatorsStaleKeyWarning
      expr: last_rotated_days{user_type="Operator"} >= 300 and last_rotated_days{user_type="Operator"} < 360
      labels:
        service: aws-iam
        severity: warning
      annotations:
        summary: IAM key for {$labels.user} will be expired within the next 60 days
        description: "For Operators if the expiration is within 15 days\n"
    - alert: PlatformApplicationStaleKeyWarning
      expr: last_rotated_days{user_type="Platform"} >= 300 and last_rotated_days{user_type="Platform"} < 360 or last_rotated_days{user_type="Application"} >= 300 and last_rotated_days{user_type="Application"} < 360
      labels:
        severity: warning
      annotations:
        summary: IAM key for Platform or Application {$labels.user} will be expired within the next 60 days
        description: "For Platform or Applications if the expiration is within 85 days\n"
    - alert: OperatorsStaleKeyViolation
      expr: last_rotated_days{user_type="Operator"} >= 360
      labels:
        severity: critical
      annotations:
        summary: IAM key for {$labels.user} is now expired
        description: "For Operators if the key is expired\n"
    - alert: PlatformApplicationStaleKeyViolation
      expr: last_rotated_days{user_type="Platform"} >= 360 or last_rotated_days{user_type="Application"} >= 360
      labels:
        severity: critical
      annotations:
        summary: IAM key for {$labels.user} is now expired
        description: "For Platform or Applications if the key is expired\n"

# CloudWatch logs alerts
- type: replace
  path: /instance_groups/name=prometheus/jobs/name=prometheus2/properties/prometheus/custom_rules?/-
  value:
    name: aws-logs
    rules:
    - alert: AWSLogsCheckNotRunning
      expr: time() - push_time_seconds{job=~"^awslogs_instance"} > 7200
      labels:
        service: aws-logs
        severity: critical
      annotations:
        summary: AWS Log Checker has not run in two hours
        description: Check https://ci.fr.cloud.gov/teams/main/pipelines/deploy-prometheus?groups=checks for failures
    - alert: AWSLogsInstanceExpired
      expr: awslogs_instance_not_logging > 0
      labels:
        service: aws-logs
        severity: warning
      annotations:
        summary: EC2 instance {{$labels.instance_id}} is not logging
        description: EC2 instance {{$labels.instance_id}} is not logging

# BOSH unknown instance alerts
- type: replace
  path: /instance_groups/name=prometheus/jobs/name=prometheus2/properties/prometheus/custom_rules?/-
  value:
    name: bosh-unknown-instances
    rules:
    - alert: BoshUnknownInstanceCheckExpired
      expr: time() - push_time_seconds{job="bosh_unknown_instance"} > 7200
      labels:
        service: bosh-unknown-iaas-instance
        severity: critical
      annotations:
        summary: Bosh Unknown Instance Check for VPC {{$labels.instance}} has not run in two hours
        description: Check bosh director {{$labels.instance}} for errors
    - alert: BoshUnknownInstanceExpired
      expr: bosh_unknown_iaas_instance > 0
      for: 5m
      labels:
        service: bosh-unknown-iaas-instance
        severity: critical
      annotations:
        summary: Bosh found unknown IaaS instance {{$labels.iaas_id}} in VPC {{$labels.vpc_name}}
        description: Unknown IaaS instance {{$labels.iaas_id}} with bosh id {{$labels.bosh_id}} found in VPC {{$labels.vpc_name}}

# Logsearch backup alerts
- type: replace
  path: /instance_groups/name=prometheus/jobs/name=prometheus2/properties/prometheus/custom_rules?/-
  value:
    name: logsearch-backup
    rules:
    - alert: LogsearchBackupFileSize
      expr: logsearch_backup_file_size < ((logsearch_file_size_threshold))
      for: 30m
      labels:
        service: logsearch-backup
        severity: warning
      annotations:
        summary: Logsearch backup file size on environment {{$labels.environment}} is too low
        description: Logsearch backup file size on environment {{$labels.environment}} has been too low for 30m.  This indicates that the log archival process to s3 may be broken.  See the archiver instances in the logsearch deployment.
    - alert: LogsearchBackupLogCount
      expr: logsearch_backup_log_count < ((logsearch_log_count_threshold))
      for: 30m
      labels:
        service: logsearch-backup
        severity: warning
      annotations:
        summary: Logsearch log count on environment {{$labels.environment}} is too low
        description: Logsearch log count on environment {{$labels.environment}} has been too low for 30m.  This indicates that we may have a logging outage.  See the ingestor processes on the ingestor instances in the logsearch deployment.  They may need restarting.
    - alert: LogsearchBackupRatio
      expr: logsearch_backup_file_size / logsearch_backup_log_count < ((logsearch_ratio_threshold))
      for: 30m
      labels:
        service: logsearch-backup
        severity: warning
      annotations:
        summary: Logsearch archive to index ratio on environment {{$labels.environment}} is too low
        description: Logsearch archive to index ratio on environment {{$labels.environment}} has been too low for 30m.  The S3 archival process or the ES ingest process may not be working.  See the archival and ingestor instances in the logsearch deployment in case any of their components need restarting.
    - alert: LogsearchBackupExpired
      # TODO: get the backup checks running on better time, then change this back to 60 * 30
      expr: time() - max(logsearch_backup_timestamp) > 60 * 45
      for: 5m
      labels:
        service: logsearch-backup
        severity: warning
      annotations:
        summary: Logsearch timestamp on environment {{$labels.environment}} is too old
        description: Logsearch timestamp on environment {{$labels.environment}} has been too old for 5m.  This indicates that the logsearch checks in https://github.com/cloud-gov/cg-deploy-logsearch/blob/main/ci/check-backup.sh have not run recently, so we do not have good data to alert on.

# Nessus alerts
- type: replace
  path: /instance_groups/name=prometheus/jobs/name=prometheus2/properties/prometheus/custom_rules?/-
  value:
    name: nessus
    rules:
    - alert: NessusManagerLicenseInvalid
      expr: nessus_manager_license_invalid != 0
      labels:
        service: nessus-manager
        severity: critical
      annotations:
        summary: Nessus Manager license is invalid
        description: https://cloud.gov/docs/ops/runbook/troubleshooting-nessus/
    - alert: NessusPluginsOutdated
      expr: nessus_manager_plugin_age > 7
      labels:
        service: nessus-manager
        severity: critical
      annotations:
        summary: Nessus Manager plugins are more than 7 days old
        description: https://cloud.gov/docs/ops/runbook/troubleshooting-nessus/
    - alert: NessusScandeleteFailing
      expr: time() - nessus_manager_scandelete_time > 172800
      labels:
        service: nessus-manager
        severity: warning
      annotations:
        summary: Nessus Manager has not been able to delete scans in 2 days
        description: https://cloud.gov/docs/ops/runbook/troubleshooting-nessus/

# UAA Client Audit alerts
- type: replace
  path: /instance_groups/name=prometheus/jobs/name=prometheus2/properties/prometheus/custom_rules?/-
  value:
    name: uaa-client-audit
    rules:
    - alert: UAAClientAuditUnexpectedClient
      expr: uaa_client_audit != 1
      labels:
        service: uaa-client-audit
        severity: warning
        uaa: '{{$labels.uaa_url}}'
      annotations:
        summary: UAA Client {{$labels.instance}} is not expected for {{$labels.uaa_url}}
        description: UAA Client {{$labels.instance}} exists but is not expected for {{$labels.uaa_url}}
    - alert: UAAClientAuditNotRunning
      expr: time() - push_time_seconds{job="uaa_client_audit"} > 7200
      labels:
        service: uaa-client-audit
        severity: warning
        uaa: '{{$labels.uaa_url}}'
      annotations:
        summary: UAA Client Audit has not run in two hours
        description: Check https://ci.fr.cloud.gov/teams/main/pipelines/deploy-cf-deployment for failures

# UAA Monitor Account Creation alerts
- type: replace
  path: /instance_groups/name=prometheus/jobs/name=prometheus2/properties/prometheus/custom_rules?/-
  value:
    name: uaa-monitor-account-creation
    rules:
    - alert: UAAMonitorAccountCreation
      expr: uaa_monitor_account_creation > 50
      labels:
        service: uaa-monitor-account-creation
        severity: warning
        uaa: '{{$labels.uaa_url}}'
      annotations:
        summary: UAA has created more accounts than expected in the past four days for {{$labels.uaa_url}}.
        description: UAA has created many more new accounts than normal. Verify the spike in new UAA accounts are legitimate.

# Concourse alerts
- type: replace
  path: /instance_groups/name=prometheus/jobs/name=prometheus2/properties/prometheus/custom_rules?/-
  value:
    name: concourse
    rules:
    - alert: ConcourseWithoutAuthentication
      expr: concourse_has_auth != 1
      labels:
        service: concourse
        severity: warning
        concourse: '{{$labels.concourse_url}}'
      annotations:
        summary: Concourse has no authentication on {{$labels.concourse_url}}
        description: Check Concourse {{$labels.concourse_url}} configuration for authentication of team {{$labels.team}}
    - alert: ConcourseCheckNotRunning
      expr: time() - push_time_seconds{job=~"concourse_.*"} > 7200
      labels:
        service: concourse
        severity: warning
      annotations:
        summary: Concourse check {{$labels.job}} is not running
        description: Check https://ci.fr.cloud.gov/teams/main/pipelines/deploy-prometheus?groups=checks for failures

# Domain broker alerts
- type: replace
  path: /instance_groups/name=prometheus/jobs/name=prometheus2/properties/prometheus/custom_rules?/-
  value:
    name: domain-broker
    rules:
    - alert: DomainBrokerNotEnoughALBs
      expr: (domain_broker_listener_count * 25) - domain_broker_certificate_count < 5
      labels:
        service: domain-broker
        severity: warning
      annotations:
        summary: 'External domain broker has too few available certificates: {{$value}}'
        description: Provision more load balancers for domain broker in Terraform and update external-domain-broker config with new ALB
    - alert: DomainBrokerFullALB
      expr: alb_listener_certificate_count > 23
      labels:
        service: domain-broker
        severity: warning
      annotations:
        summary: 'Alb listener {{$labels.alb_arn}} has too many certificates'
        description: "If DomainBrokerNotEnoughALBs isn't firing, the external-domain-broker is probably either placing instances incorrectly or not cleaning up certificates."
    - alert: DomainBrokerServiceMultipleCertificates
      expr: service_instance_duplicate_cert_count > 0
      labels:
        service: domain-broker
        severity: warning
      annotations:
        summary: 'Service instance {{$labels.service_instance_id}} has too many certificates'
        description: "There are multiple certificates for the given service instance."

- type: replace
  path: /instance_groups/name=prometheus/jobs/name=prometheus2/properties/prometheus/custom_rules?/-
  value:
    name: blackbox
    rules:
    - alert: SSLCertExpiringSoon
      expr: probe_ssl_earliest_cert_expiry{job="blackbox"} - time() < 86400 * 25
      for: 10m
      labels:
        service: blackbox
        severity: warning
      annotations:
        summary: SSL certificate for {{$labels.instance}} is expiring within 25d
        description: Renew SSL certificate for {{$labels.instance}}
    - alert: ProbeFailing
      expr: probe_success{job="blackbox"} < 1
      for: 5m
      labels:
        service: blackbox
        severity: warning
      annotations:
        summary: Probe failing for {{$labels.instance}} for 5m
        description: Check health of {{$labels.instance}}

# CF Core - non-customer stuff.
- type: replace
  path: /instance_groups/name=prometheus/jobs/name=prometheus2/properties/prometheus/custom_rules?/-
  value:
    name: cg-operators
    rules:
    - alert: CFOperatorAppUnhealthy
      expr: min(cf_application_instances{organization_id="4175a5b2-78a4-42b0-94c2-280f8be41e7f"} - cf_application_instances_running{state=~"STARTED"}) by(environment, deployment, organization_name, space_name, application_name) > 0
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "CF Application `{{$labels.environment}}/{{$labels.deployment}}/{{$labels.organization_name}}/{{$labels.space_name}}/{{$labels.application_name}}` is unhealthy"
        description: "CF Application `{{$labels.application_name}}` at environment `{{$labels.environment}}`, deployment `{{$labels.deployment}}`, org `{{$labels.organization_name}}`, and space `{{$labels.space_name}}` isn't in good shape, you might want to take a look."
    - alert: CFOperatorAppCrashed
      expr: sum(cf_application_instances_running{state=~"STARTED", organization_id="4175a5b2-78a4-42b0-94c2-280f8be41e7f"}) by(environment, deployment, organization_name, space_name, application_name) == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "CF Application `{{$labels.environment}}/{{$labels.deployment}}/{{$labels.organization_name}}/{{$labels.space_name}}/{{$labels.application_name}}` does not have any instance running"
        description: "CF Application `{{$labels.application_name}}` at environment `{{$labels.environment}}`, deployment `{{$labels.deployment}}`, org `{{$labels.organization_name}}`, and space `{{$labels.space_name}}` has not had any instance running. You might want to look into that."


# Network watchers
- type: replace
  path: /instance_groups/name=prometheus/jobs/name=prometheus2/properties/prometheus/custom_rules?/-
  value:
    name: networker
    rules:
    - alert: NetworkTrafficBurst
      expr: avg by (environment) (irate(firehose_counter_event_gorouter_total_requests_total{bosh_job_name="router"}[5m] offset 5m)) / avg by (environment) (irate(firehose_counter_event_gorouter_total_requests_total{bosh_job_name="router"}[5m])) >= 1.40
      labels:
        severity: warning
      annotations:
        summary: Burst in incoming network traffic
        description: "The platform has seen a burst in incoming network traffic, please review this dashboard: https://grafana.fr.cloud.gov/d/cf_router/cf-router"
    - alert: LargeNetworkTrafficBurst
      expr: avg by (environment) (irate(firehose_counter_event_gorouter_total_requests_total{bosh_job_name="router"}[5m] offset 5m)) / avg by (environment) (irate(firehose_counter_event_gorouter_total_requests_total{bosh_job_name="router"}[5m])) >= 1.50
      labels:
        severity: critical
      annotations:
        summary: Burst in incoming network traffic
        description: "The platform has seen a very large burst in incoming network traffic, please review this dashboard: https://grafana.fr.cloud.gov/d/cf_router/cf-router"

- type: replace
  path: /instance_groups/name=prometheus/jobs/name=prometheus2/properties/prometheus/custom_rules?/-
  value:
    name: aide
    rules:
    - alert: AideViolations
      expr: aide_violation_count > 0
      labels:
        service: aide
        severity: warning
      annotations:
        summary: 'AIDE found violations for {{$labels.instance}}, with action {{$labels.action}}'
        description: Review AIDE report in logs-platform - if changes are expected, update AIDE database on alerting VM

- type: replace
  path: /instance_groups/name=prometheus/jobs/name=prometheus2/properties/prometheus/custom_rules?/-
  value:
    name: alb-certificate-expiration
    rules:
    - alert: AlbCertificateExpiring
      # Reducing the time on the left side of the comparator *should* make this more human readable
      expr: (domain_broker_certificate_expiration - time())/(24*60*60) < 25
      labels:
        service: ALB
        severity: critical
      annotations:
        summary: '{{$labels.certificate_name}}, expires in less than 15 days. Days until expiration: {{$value}}'
        description: Find out why the certificate is not renewing. The service instance ID is probably in the certificate name.

- type: replace
  path: /instance_groups/name=prometheus/jobs/name=prometheus2/properties/prometheus/custom_rules?/-
  value:
    name: cdn-certificate-expiration
    rules:
    - alert: CDNCertificateExpiring
      expr: cdn_certificate_expiration < 25
      labels:
        service: CDN
        severity: critical
      annotations:
        summary: '{{$labels.arn}} with alias {{$labels.alias}} and cloudfront domain {{$labels.domain}}, expires in less than 25 days. Days until expiration: {{$value}}'
        description: 'Find out why the certificate is not renewing. The CloudFront instance ID is {{$labels.id}}.'

# ClamAV Alerts
- type: replace
  path: /instance_groups/name=prometheus/jobs/name=prometheus2/properties/prometheus/custom_rules?/-
  value:
    name: clamav
    rules:
    - alert: ClamAV Alert
      expr: clamav_virus_detected > 0
      labels:
        service: clamav
        severity: warning
      annotations:
        summary: ClamAV scan alert for {{$labels.instance}} in {{$labels.environment}}/{{$labels.deployment}}
        description: ClamAV alert for {{$labels.instance}} in {{$labels.environment}}/{{$labels.deployment}} - check the logs on the instance to establish threat and follow procedures

# Snort Alerts
- type: replace
  path: /instance_groups/name=prometheus/jobs/name=prometheus2/properties/prometheus/custom_rules?/-
  value:
    name: snort
    rules:
    - alert: Snort Alert
      expr: snort_alert_detected > 0
      labels:
        service: snort
        severity: warning
      annotations:
        summary: Snort rule alert for {{$labels.instance}} in {{$labels.environment}}/{{$labels.deployment}}
        description: Snort alert for {{$labels.instance}} in {{$labels.environment}}/{{$labels.deployment}} - check the auth.log on the instance to establish threat and follow procedures

# HTTPS uptime alerts
- type: replace
  path: /instance_groups/name=prometheus/jobs/name=prometheus2/properties/prometheus/custom_rules?/-
  value:
    name: uptime-monitor
    rules:
    - alert: UptimeMonitorFailed
      expr: probe_http_status_code{job="blackbox"} != 200
      for: 5m
      labels:
        service: uptime
        severity: warning
      annotations:
        summary: Uptime check failed (non-200 response) for {{$labels.instance}} in {{$labels.environment}}/{{$labels.deployment}}
        description: Uptime check failed (non-200 response) for {{$labels.instance}} in {{$labels.environment}}/{{$labels.deployment}}. Runbook - https://github.com/cloud-gov/internal-docs/blob/main/docs/runbooks/Metrics-and-Alerting/uptime-monitoring.md#uptimemonitorfailed

# opensearch disk alerts
- type: replace
  path: /instance_groups/name=prometheus/jobs/name=prometheus2/properties/prometheus/custom_rules?/-
  value:
    name: opensearch
    rules:
    - alert: opensearch_disk_critical
      expr: bosh_job_persistent_disk_percent{bosh_deployment="logs-opensearch"} > 82
      for: 30m
      labels:
        service: opensearch
        severity: critical
      annotations:
        summary: "opensearch node {{$labels.bosh_job_name}}/{{$labels.bosh_job_id}} index {{$labels.bosh_job_index}} disk is over 85% used"
        description: Review deployment and indexes and see if index and/or scaling operations need to take place for growth
    - alert: opensearch_disk_warning
      expr: bosh_job_persistent_disk_percent{bosh_deployment="logs-opensearch"} > 85
      for: 5m
      labels:
        service: opensearch
        severity: warning
      annotations:
        summary: "opensearch node {{$labels.bosh_job_name}}/{{$labels.bosh_job_id}} index {{$labels.bosh_job_index}} disk is over 88% used"
        description: Review deployment and indexes and see if index and/or scaling operations need to take place for growth
