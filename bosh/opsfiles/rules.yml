# AWS IAM MFA alerts
- type: replace
  path: /instance_groups/name=prometheus/jobs/name=prometheus2/properties/prometheus/custom_rules?/-
  value:
    name: aws-mfa
    rules:
    - alert: AWSMFADisabled
      expr: aws_iam_user_mfa != 1
      labels:
        service: aws-iam
        severity: warning
      annotations:
        summary: AWS IAM user {{$labels.instance}} does not have MFA enabled
        description: Remind user {{$labels.instance}} to enable MFA
    - alert: AWSMFANotRunning
      expr: time() - push_time_seconds{job="aws_iam"} > 7200
      labels:
        service: aws-iam
        severity: warning
      annotations:
        summary: AWS IAM check is not running
        description: Check https://ci.fr.cloud.gov/teams/main/pipelines/deploy-prometheus?groups=checks for failures

# AWS RDS storage alerts
- type: replace
  path: /instance_groups/name=prometheus/jobs/name=prometheus2/properties/prometheus/custom_rules?/-
  value:
    name: aws-rds-storage
    rules:
    - alert: AWSRDSStorage
      expr: aws_rds_disk_free < (aws_rds_disk_allocated * .12)
      labels:
        service: aws-rds
        severity: warning
      annotations:
        summary: AWS RDS {{$labels.instance}} has used 88% or greater disk space
        description: Email the organization administrator for {{$labels.instance}} to inquire about provisioning a larger database.

# CloudWatch logs alerts
- type: replace
  path: /instance_groups/name=prometheus/jobs/name=prometheus2/properties/prometheus/custom_rules?/-
  value:
    name: aws-logs
    rules:
    - alert: AWSLogsGroupExpired
      expr: awslogs_loggroup_not_logging > 0
      labels:
        service: aws-logs
        severity: warning
      annotations:
        summary: CloudWatch Logs group {{$labels.group}} is not receiving logs
        description: CloudWatch Logs group {{$labels.group}} has not received a log in the last hour
    - alert: AWSLogsCheckNotRunning
      expr: time() - push_time_seconds{job=~"^awslogs.*"} > 7200
      labels:
        service: aws-logs
        severity: critical
      annotations:
        summary: AWS Log Checker has not run in two hours
        description: Check https://ci.fr.cloud.gov/teams/main/pipelines/deploy-prometheus?groups=checks for failures
    - alert: AWSLogsInstanceExpired
      expr: awslogs_instance_not_logging > 0
      labels:
        service: aws-logs
        severity: warning
      annotations:
        summary: EC2 instance {{$labels.instance_id}} is not logging
        description: EC2 instance {{$labels.instance_id}} is not logging
    - alert: AWSLogsInstanceStopping
      expr: awslogs_instance_not_logging > 1
      labels:
        service: aws-logs
        severity: critical
      annotations:
        summary: EC2 instance {{$labels.instance_id}} will be stopped due to not logging
        description: EC2 instance {{$labels.instance_id}} will be stopped due to not logging

# BOSH unknown instance alerts
- type: replace
  path: /instance_groups/name=prometheus/jobs/name=prometheus2/properties/prometheus/custom_rules?/-
  value:
    name: bosh-unknown-instances
    rules:
    - alert: BoshUnknownInstanceCheckExpired
      expr: time() - push_time_seconds{job="bosh_unknown_instance"} > 7200
      labels:
        service: bosh-unknown-iaas-instance
        severity: critical
      annotations:
        summary: Bosh Unknown Instance Check for VPC {{$labels.instance}} has not run in two hours
        description: Check bosh director {{$labels.instance}} for errors
    - alert: BoshUnknownInstanceExpired
      expr: bosh_unknown_iaas_instance > 0
      labels:
        service: bosh-unknown-iaas-instance
        severity: critical
      annotations:
        summary: Bosh found unknown IaaS instance {{$labels.iaas_id}} in VPC {{$labels.vpc_name}}
        description: Unknown IaaS instance {{$labels.iaas_id}} with bosh id {{$labels.bosh_id}} found in VPC {{$labels.vpc_name}}

# Kubernetes broker alerts
- type: replace
  path: /instance_groups/name=prometheus/jobs/name=prometheus2/properties/prometheus/custom_rules?/-
  value:
    name: kubernetes-broker
    rules:
    - alert: BrokeredElasticsearchAlive
      expr: kubernetes_broker_elasticsearch_alive != 1
      for: 5m
      labels:
        service: kubernetes-broker-elasticsearch
        severity: warning
      annotations:
        summary: Brokered elasticsearch instance {{$labels.instance_guid}} is not responding
        description: Brokered elasticsearch instance {{$labels.instance_guid}} has not been responding for 5m
    - alert: BrokeredElasticsearchHealthy
      expr: kubernetes_broker_elasticsearch_healthy != 1
      for: 5m
      labels:
        service: kubernetes-broker-elasticsearch
        severity: warning
      annotations:
        summary: Brokered elasticsearch instance {{$labels.instance_guid}} is not healthy
        description: Brokered elasticsearch instance {{$labels.instance_guid}} has not been healthy for 5m
    - alert: BrokeredRedisAlive
      expr: kubernetes_broker_redis_alive != 1
      for: 5m
      labels:
        service: kubernetes-broker-redis
        severity: warning
      annotations:
        summary: Brokered redis instance {{$labels.instance_guid}} is not responding
        description: Brokered redis instance {{$labels.instance_guid}} has not been responding for 5m
    - alert: BrokeredRedisHealthy
      expr: kubernetes_broker_redis_healthy != 1
      for: 5m
      labels:
        service: kubernetes-broker-redis
        severity: warning
      annotations:
        summary: Brokered redis instance {{$labels.instance_guid}} is not healthy
        description: Brokered redis instance {{$labels.instance_guid}} has not been healthy for 5m

# Logsearch backup alerts
- type: replace
  path: /instance_groups/name=prometheus/jobs/name=prometheus2/properties/prometheus/custom_rules?/-
  value:
    name: logsearch-backup
    rules:
    - alert: LogsearchBackupFileSize
      expr: logsearch_backup_file_size < ((logsearch_file_size_threshold))
      for: 30m
      labels:
        service: logsearch-backup
        severity: warning
      annotations:
        summary: Logsearch backup file size on environment {{$labels.environment}} is too low
        description: Logsearch backup file size on environment {{$labels.environment}} has been too low for 30m.  This indicates that the log archival process to s3 may be broken.  See the archiver instances in the logsearch deployment.
    - alert: LogsearchBackupLogCount
      expr: logsearch_backup_log_count < ((logsearch_log_count_threshold))
      for: 30m
      labels:
        service: logsearch-backup
        severity: warning
      annotations:
        summary: Logsearch log count on environment {{$labels.environment}} is too low
        description: Logsearch log count on environment {{$labels.environment}} has been too low for 30m.  This indicates that we may have a logging outage.  See the ingestor processes on the ingestor instances in the logsearch deployment.  They may need restarting.
    - alert: LogsearchBackupRatio
      expr: logsearch_backup_file_size / logsearch_backup_log_count < ((logsearch_ratio_threshold))
      for: 30m
      labels:
        service: logsearch-backup
        severity: warning
      annotations:
        summary: Logsearch archive to index ratio on environment {{$labels.environment}} is too low
        description: Logsearch archive to index ratio on environment {{$labels.environment}} has been too low for 30m.  The S3 archival process or the ES ingest process may not be working.  See the archival and ingestor instances in the logsearch deployment in case any of their components need restarting.
    - alert: LogsearchBackupExpired
      expr: time() - max(logsearch_backup_timestamp) > 60 * 30
      for: 5m
      labels:
        service: logsearch-backup
        severity: warning
      annotations:
        summary: Logsearch timestamp on environment {{$labels.environment}} is too old
        description: Logsearch timestamp on environment {{$labels.environment}} has been too old for 5m.  This indicates that the logsearch checks in https://github.com/18F/cg-deploy-logsearch/blob/master/ci/check-backup.sh have not run recently, so we do not have good data to alert on.

# Nessus alerts
- type: replace
  path: /instance_groups/name=prometheus/jobs/name=prometheus2/properties/prometheus/custom_rules?/-
  value:
    name: nessus
    rules:
    - alert: NessusManagerLicenseInvalid
      expr: nessus_manager_license_invalid != 0
      labels:
        service: nessus-manager
        severity: critical
      annotations:
        summary: Nessus Manager license is invalid
        description: https://cloud.gov/docs/ops/runbook/troubleshooting-nessus/
    - alert: NessusPluginsOutdated
      expr: nessus_manager_plugin_age > 7
      labels:
        service: nessus-manager
        severity: critical
      annotations:
        summary: Nessus Manager plugins are more than 7 days old
        description: https://cloud.gov/docs/ops/runbook/troubleshooting-nessus/
    - alert: NessusScandeleteFailing
      expr: time() - nessus_manager_scandelete_time > 172800
      labels:
        service: nessus-manager
        severity: warning
      annotations:
        summary: Nessus Manager has not been able to delete scans in 2 days
        description: https://cloud.gov/docs/ops/runbook/troubleshooting-nessus/


# Kubernetes etcd alerts
# Note: Only the leader emits metrics, so take max to ignore stale metrics
- type: replace
  path: /instance_groups/name=prometheus/jobs/name=prometheus2/properties/prometheus/custom_rules?/-
  value:
    name: kubernetes-backup
    rules:
    - alert: KubernetesEtcdBackupExpired
      expr: time() - max(kubernetes_etcd_backup) by (environment) > 1800
      labels:
        service: kubernetes-etcd
        severity: warning
      annotations:
        summary: Kubernetes etcd backup is out of date
        description: Check kubernetes etcd backup

# UAA Client Audit alerts
- type: replace
  path: /instance_groups/name=prometheus/jobs/name=prometheus2/properties/prometheus/custom_rules?/-
  value:
    name: uaa-client-audit
    rules:
    - alert: UAAClientAuditUnexpectedClient
      expr: uaa_client_audit != 1
      labels:
        service: uaa-client-audit
        severity: warning
        uaa: '{{$labels.uaa_url}}'
      annotations:
        summary: UAA Client {{$labels.instance}} is not expected for {{$labels.uaa_url}}
        description: UAA Client {{$labels.instance}} exists but is not expected for {{$labels.uaa_url}}
    - alert: UAAClientAuditNotRunning
      expr: time() - push_time_seconds{job="uaa_client_audit"} > 7200
      labels:
        service: uaa-client-audit
        severity: warning
        uaa: '{{$labels.uaa_url}}'
      annotations:
        summary: UAA Client Audit has not run in two hours
        description: Check https://ci.fr.cloud.gov/teams/main/pipelines/deploy-cf-deployment for failures

# Concourse alerts
- type: replace
  path: /instance_groups/name=prometheus/jobs/name=prometheus2/properties/prometheus/custom_rules?/-
  value:
    name: concourse
    rules:
    - alert: ConcourseWithoutAuthentication
      expr: concourse_has_auth != 1
      labels:
        service: concourse
        severity: warning
        concourse: '{{$labels.concourse_url}}'
      annotations:
        summary: Concourse has no authentication on {{$labels.concourse_url}}
        description: Check Concourse {{$labels.concourse_url}} configuration for authentication of team {{$labels.team}}
    - alert: ConcourseCheckNotRunning
      expr: time() - push_time_seconds{job=~"concourse_.*"} > 7200
      labels:
        service: concourse
        severity: warning
      annotations:
        summary: Concourse check {{$labels.job}} is not running
        description: Check https://ci.fr.cloud.gov/teams/main/pipelines/deploy-prometheus?groups=checks for failures

# Domain broker alerts
- type: replace
  path: /instance_groups/name=prometheus/jobs/name=prometheus2/properties/prometheus/custom_rules?/-
  value:
    name: domain-broker
    rules:
    - alert: DomainBrokerNotEnoughALBs
      expr: domain_broker_certificate_count / domain_broker_listener_count > 20
      labels:
        service: domain-broker
        severity: warning
      annotations:
        summary: 'Domain broker {{$labels.instance}} is close to the certificates per listener limit: {{$value}}'
        description: Provision more load balancers for domain broker {{$value}}

- type: replace
  path: /instance_groups/name=prometheus/jobs/name=prometheus2/properties/prometheus/custom_rules?/-
  value:
    name: blackbox
    rules:
    - alert: SSLCertExpiringSoon
      expr: probe_ssl_earliest_cert_expiry{job="blackbox"} - time() < 86400 * 25
      for: 10m
      labels:
        service: blackbox
        severity: warning
      annotations:
        summary: SSL certificate for {{$labels.instance}} is expiring within 25d
        description: Renew SSL certificate for {{$labels.instance}}
    - alert: ProbeFailing
      expr: probe_success{job="blackbox"} < 1
      for: 5m
      labels:
        service: blackbox
        severity: warning
      annotations:
        summary: Probe failing for {{$labels.instance}} for 5m
        description: Check health of {{$labels.instance}}

# Tripwire alerts
- type: replace
  path: /instance_groups/name=prometheus/jobs/name=prometheus2/properties/prometheus/custom_rules?/-
  value:
    name: tripwire
    rules:
    - alert: TripwireViolation
      expr: tripwire_violation_count > 0
      labels:
        service: tripwire
        severity: warning
      annotations:
        summary: 'Tripwire found violations for {{$labels.instance}}, with action {{$labels.action}} in section {{$labels.section}}'
        description: Review Tripwire report in CloudWatch logs

# CF Core - non-customer stuff.
- alert: CFOperatorAppUnhealthy
  expr: min(cf_application_instances{state=~"STARTED", organization_id="4175a5b2-78a4-42b0-94c2-280f8be41e7f"} - cf_application_instances_running{state=~"STARTED"}) by(environment, deployment, organization_name, space_name, application_name) > 0
  for: <%= p('cloudfoundry_alerts.app_unhealthy.evaluation_time') %>
  labels:
    service: <%= p('cloudfoundry_alerts.app_service_name') %>
    severity: warning
  annotations:
    summary: "CF Application `{{$labels.environment}}/{{$labels.deployment}}/{{$labels.organization_name}}/{{$labels.space_name}}/{{$labels.application_name}}` is unhealthy"
    description: "CF Application `{{$labels.application_name}}` at environment `{{$labels.environment}}`, deployment `{{$labels.deployment}}`, org `{{$labels.organization_name}}`, and space `{{$labels.space_name}}` had less instances running than desired during the last <%= p('cloudfoundry_alerts.app_unhealthy.evaluation_time') %>"
- alert: CFOperatorAppCrashed
  expr: sum(cf_application_instances_running{state=~"STARTED", organization_id="4175a5b2-78a4-42b0-94c2-280f8be41e7f"}) by(environment, deployment, organization_name, space_name, application_name) == 0
  for: <%= p('cloudfoundry_alerts.app_crashed.evaluation_time') %>
  labels:
    service: <%= p('cloudfoundry_alerts.app_service_name') %>
    severity: critical
  annotations:
    summary: "CF Application `{{$labels.environment}}/{{$labels.deployment}}/{{$labels.organization_name}}/{{$labels.space_name}}/{{$labels.application_name}}` does not have any instance running"
    description: "CF Application `{{$labels.application_name}}` at environment `{{$labels.environment}}`, deployment `{{$labels.deployment}}`, org `{{$labels.organization_name}}`, and space `{{$labels.space_name}}` has not had any instance running during the last <%= p('cloudfoundry_alerts.app_crashed.evaluation_time') %>"